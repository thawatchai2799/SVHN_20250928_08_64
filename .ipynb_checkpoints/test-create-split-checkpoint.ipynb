{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a31b17e-7f92-49f5-b06c-4d90415c232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run 000 of 001 ---\n",
      "Running: Activation=GELU at 2025-09-28 04:29:14\n",
      "Epoch 1/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.1855 - loss: 2.2451 - val_accuracy: 0.1990 - val_loss: 2.2041\n",
      "Epoch 2/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1972 - loss: 2.1983 - val_accuracy: 0.2198 - val_loss: 2.1515\n",
      "Epoch 3/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2153 - loss: 2.1386 - val_accuracy: 0.2277 - val_loss: 2.0852\n",
      "Epoch 4/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2212 - loss: 2.0986 - val_accuracy: 0.2401 - val_loss: 2.0640\n",
      "Epoch 5/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2271 - loss: 2.0855 - val_accuracy: 0.2417 - val_loss: 2.0528\n",
      "Epoch 6/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2284 - loss: 2.0780 - val_accuracy: 0.2396 - val_loss: 2.0548\n",
      "Epoch 7/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2306 - loss: 2.0709 - val_accuracy: 0.2442 - val_loss: 2.0554\n",
      "Epoch 8/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2344 - loss: 2.0665 - val_accuracy: 0.2440 - val_loss: 2.0413\n",
      "Epoch 9/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2326 - loss: 2.0664 - val_accuracy: 0.2445 - val_loss: 2.0375\n",
      "Epoch 10/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2357 - loss: 2.0618 - val_accuracy: 0.2420 - val_loss: 2.0397\n",
      "Epoch 11/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2356 - loss: 2.0582 - val_accuracy: 0.2464 - val_loss: 2.0367\n",
      "Epoch 12/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2418 - loss: 2.0514 - val_accuracy: 0.2303 - val_loss: 2.0671\n",
      "Epoch 13/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2413 - loss: 2.0512 - val_accuracy: 0.2306 - val_loss: 2.0581\n",
      "Epoch 14/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2393 - loss: 2.0479 - val_accuracy: 0.2458 - val_loss: 2.0251\n",
      "Epoch 15/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2439 - loss: 2.0460 - val_accuracy: 0.2472 - val_loss: 2.0334\n",
      "Epoch 16/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2445 - loss: 2.0394 - val_accuracy: 0.2497 - val_loss: 2.0181\n",
      "Epoch 17/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2467 - loss: 2.0308 - val_accuracy: 0.2203 - val_loss: 2.0924\n",
      "Epoch 18/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2457 - loss: 2.0295 - val_accuracy: 0.2511 - val_loss: 2.0212\n",
      "Epoch 19/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2491 - loss: 2.0210 - val_accuracy: 0.2403 - val_loss: 2.0421\n",
      "Epoch 20/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2525 - loss: 2.0086 - val_accuracy: 0.2517 - val_loss: 2.0097\n",
      "Epoch 21/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2509 - loss: 2.0102 - val_accuracy: 0.2566 - val_loss: 2.0053\n",
      "Epoch 22/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2544 - loss: 2.0031 - val_accuracy: 0.2597 - val_loss: 2.0023\n",
      "Epoch 23/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2551 - loss: 2.0037 - val_accuracy: 0.2543 - val_loss: 2.0227\n",
      "Epoch 24/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2546 - loss: 2.0003 - val_accuracy: 0.2620 - val_loss: 1.9998\n",
      "Epoch 25/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2559 - loss: 2.0038 - val_accuracy: 0.2604 - val_loss: 2.0048\n",
      "Epoch 26/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2577 - loss: 1.9946 - val_accuracy: 0.2619 - val_loss: 1.9986\n",
      "Epoch 27/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2581 - loss: 1.9960 - val_accuracy: 0.2581 - val_loss: 1.9983\n",
      "Epoch 28/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2565 - loss: 1.9983 - val_accuracy: 0.2557 - val_loss: 2.0134\n",
      "Epoch 29/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2589 - loss: 1.9910 - val_accuracy: 0.2636 - val_loss: 1.9922\n",
      "Epoch 30/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2622 - loss: 1.9925 - val_accuracy: 0.2539 - val_loss: 2.0308\n",
      "Epoch 31/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2593 - loss: 1.9958 - val_accuracy: 0.2611 - val_loss: 1.9976\n",
      "Epoch 32/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2624 - loss: 1.9858 - val_accuracy: 0.2619 - val_loss: 2.0026\n",
      "Epoch 33/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2610 - loss: 1.9923 - val_accuracy: 0.2598 - val_loss: 2.0103\n",
      "Epoch 34/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2606 - loss: 1.9921 - val_accuracy: 0.2638 - val_loss: 1.9916\n",
      "Epoch 35/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2603 - loss: 1.9961 - val_accuracy: 0.2617 - val_loss: 1.9973\n",
      "Epoch 36/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2609 - loss: 1.9922 - val_accuracy: 0.2656 - val_loss: 1.9890\n",
      "Epoch 37/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2631 - loss: 1.9907 - val_accuracy: 0.2657 - val_loss: 1.9890\n",
      "Epoch 38/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2587 - loss: 1.9898 - val_accuracy: 0.2626 - val_loss: 1.9961\n",
      "Epoch 39/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2624 - loss: 1.9843 - val_accuracy: 0.2653 - val_loss: 1.9922\n",
      "Epoch 40/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2620 - loss: 1.9883 - val_accuracy: 0.2664 - val_loss: 1.9906\n",
      "Epoch 41/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2638 - loss: 1.9876 - val_accuracy: 0.2223 - val_loss: 2.1215\n",
      "Epoch 42/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2630 - loss: 1.9914 - val_accuracy: 0.2657 - val_loss: 1.9934\n",
      "Epoch 43/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2619 - loss: 1.9924 - val_accuracy: 0.2684 - val_loss: 1.9863\n",
      "Epoch 44/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2597 - loss: 1.9924 - val_accuracy: 0.2632 - val_loss: 1.9971\n",
      "Epoch 45/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2603 - loss: 1.9936 - val_accuracy: 0.2583 - val_loss: 2.0176\n",
      "Epoch 46/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2613 - loss: 1.9892 - val_accuracy: 0.2544 - val_loss: 2.0210\n",
      "Epoch 47/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2617 - loss: 1.9910 - val_accuracy: 0.2545 - val_loss: 2.0251\n",
      "Epoch 48/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2596 - loss: 1.9982 - val_accuracy: 0.2623 - val_loss: 1.9965\n",
      "Epoch 49/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2589 - loss: 1.9992 - val_accuracy: 0.2606 - val_loss: 2.0060\n",
      "Epoch 50/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2599 - loss: 1.9950 - val_accuracy: 0.2618 - val_loss: 2.0108\n",
      "Epoch 51/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2563 - loss: 2.0008 - val_accuracy: 0.2359 - val_loss: 2.1277\n",
      "Epoch 52/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2577 - loss: 2.0075 - val_accuracy: 0.2606 - val_loss: 2.0084\n",
      "Epoch 53/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2607 - loss: 1.9949 - val_accuracy: 0.2647 - val_loss: 1.9907\n",
      "Epoch 54/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2588 - loss: 2.0052 - val_accuracy: 0.2624 - val_loss: 2.0041\n",
      "Epoch 55/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2525 - loss: 2.0093 - val_accuracy: 0.2637 - val_loss: 1.9908\n",
      "Epoch 56/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2536 - loss: 2.0180 - val_accuracy: 0.2557 - val_loss: 2.0032\n",
      "Epoch 57/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2522 - loss: 2.0269 - val_accuracy: 0.2590 - val_loss: 2.0164\n",
      "Epoch 58/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2541 - loss: 2.0126 - val_accuracy: 0.2637 - val_loss: 1.9992\n",
      "Epoch 59/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2536 - loss: 2.0194 - val_accuracy: 0.2534 - val_loss: 2.0494\n",
      "Epoch 60/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2492 - loss: 2.0387 - val_accuracy: 0.1798 - val_loss: 2.2784\n",
      "Epoch 61/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2546 - loss: 2.0143 - val_accuracy: 0.2375 - val_loss: 2.1276\n",
      "Epoch 62/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2527 - loss: 2.0110 - val_accuracy: 0.2620 - val_loss: 2.0026\n",
      "Epoch 63/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2551 - loss: 2.0112 - val_accuracy: 0.2559 - val_loss: 2.0195\n",
      "Epoch 64/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2556 - loss: 2.0214 - val_accuracy: 0.2548 - val_loss: 2.0276\n",
      "Epoch 65/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2526 - loss: 2.0231 - val_accuracy: 0.2588 - val_loss: 1.9992\n",
      "Epoch 66/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2558 - loss: 2.0112 - val_accuracy: 0.2571 - val_loss: 2.0038\n",
      "Epoch 67/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2512 - loss: 2.0154 - val_accuracy: 0.2487 - val_loss: 2.0774\n",
      "Epoch 68/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2573 - loss: 2.0036 - val_accuracy: 0.2626 - val_loss: 2.0070\n",
      "Epoch 69/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2449 - loss: 2.0335 - val_accuracy: 0.2543 - val_loss: 2.0343\n",
      "Epoch 70/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2531 - loss: 2.0212 - val_accuracy: 0.2549 - val_loss: 2.0190\n",
      "Epoch 71/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2512 - loss: 2.0317 - val_accuracy: 0.2624 - val_loss: 1.9960\n",
      "Epoch 72/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2527 - loss: 2.0228 - val_accuracy: 0.2512 - val_loss: 2.0346\n",
      "Epoch 73/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2487 - loss: 2.0332 - val_accuracy: 0.2288 - val_loss: 2.1290\n",
      "Epoch 74/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2490 - loss: 2.0283 - val_accuracy: 0.2629 - val_loss: 1.9993\n",
      "Epoch 75/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2512 - loss: 2.0312 - val_accuracy: 0.2580 - val_loss: 2.0137\n",
      "Epoch 76/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2266 - loss: 2.1101 - val_accuracy: 0.2154 - val_loss: 2.1728\n",
      "Epoch 77/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2168 - loss: 2.1633 - val_accuracy: 0.2550 - val_loss: 2.0260\n",
      "Epoch 78/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2492 - loss: 2.0417 - val_accuracy: 0.1963 - val_loss: 2.2275\n",
      "Epoch 79/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2029 - loss: 2.1996 - val_accuracy: 0.2069 - val_loss: 2.2007\n",
      "Epoch 80/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2192 - loss: 2.1462 - val_accuracy: 0.2616 - val_loss: 2.0081\n",
      "Epoch 81/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2432 - loss: 2.0570 - val_accuracy: 0.2304 - val_loss: 2.1000\n",
      "Epoch 82/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2446 - loss: 2.0410 - val_accuracy: 0.1965 - val_loss: 2.2281\n",
      "Epoch 83/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.1902 - loss: 2.2343 - val_accuracy: 0.2278 - val_loss: 2.1337\n",
      "Epoch 84/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2407 - loss: 2.0607 - val_accuracy: 0.2207 - val_loss: 2.1405\n",
      "Epoch 85/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2466 - loss: 2.0350 - val_accuracy: 0.2189 - val_loss: 2.2180\n",
      "Epoch 86/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2428 - loss: 2.0510 - val_accuracy: 0.2430 - val_loss: 2.0723\n",
      "Epoch 87/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2061 - loss: 2.2011 - val_accuracy: 0.2235 - val_loss: 2.1518\n",
      "Epoch 88/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2261 - loss: 2.1185 - val_accuracy: 0.2189 - val_loss: 2.1505\n",
      "Epoch 89/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2440 - loss: 2.0650 - val_accuracy: 0.2340 - val_loss: 2.0858\n",
      "Epoch 90/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2443 - loss: 2.0420 - val_accuracy: 0.2371 - val_loss: 2.0936\n",
      "Epoch 91/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2413 - loss: 2.0567 - val_accuracy: 0.1968 - val_loss: 2.2288\n",
      "Epoch 92/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1933 - loss: 2.2245 - val_accuracy: 0.2203 - val_loss: 2.1614\n",
      "Epoch 93/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2297 - loss: 2.1178 - val_accuracy: 0.2594 - val_loss: 2.0152\n",
      "Epoch 94/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2461 - loss: 2.0589 - val_accuracy: 0.2416 - val_loss: 2.0613\n",
      "Epoch 95/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2445 - loss: 2.0446 - val_accuracy: 0.2019 - val_loss: 2.2302\n",
      "Epoch 96/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2034 - loss: 2.2024 - val_accuracy: 0.2149 - val_loss: 2.1772\n",
      "Epoch 97/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1937 - loss: 2.2268 - val_accuracy: 0.2117 - val_loss: 2.1869\n",
      "Epoch 98/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2096 - loss: 2.1860 - val_accuracy: 0.2183 - val_loss: 2.1467\n",
      "Epoch 99/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2426 - loss: 2.0574 - val_accuracy: 0.2112 - val_loss: 2.1895\n",
      "Epoch 100/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2343 - loss: 2.1102 - val_accuracy: 0.2011 - val_loss: 2.2168\n",
      "Epoch 101/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2206 - loss: 2.1516 - val_accuracy: 0.1748 - val_loss: 2.2637\n",
      "Running: Activation=GELU at 2025-09-28 04:33:37\n",
      "Details in results for round 000:\n",
      "\n",
      "Activation: GELU\n",
      "  accuracy: [0.18821409 0.20531826 0.21425939 0.22205387 0.22804646 0.23040801\n",
      " 0.23256481 0.23432574 0.2347489  0.2360184  0.23693299 0.240605\n",
      " 0.24068689 0.24019548 0.24343066 0.24508238 0.24596967 0.24521889\n",
      " 0.24859057 0.25153911 0.25309527 0.25428286 0.25429651 0.25570253\n",
      " 0.25622123 0.25821424 0.25889677 0.25900596 0.25900596 0.26052117\n",
      " 0.26057577 0.25987959 0.26091704 0.26086244 0.26048023 0.2612583\n",
      " 0.26139483 0.26043928 0.26248688 0.26108086 0.26151767 0.260685\n",
      " 0.26157227 0.26053482 0.26158592 0.25987959 0.25949737 0.25891042\n",
      " 0.25805041 0.26016626 0.25709489 0.25811869 0.25701299 0.25316352\n",
      " 0.25265846 0.25454223 0.25623488 0.25297242 0.25084293 0.25138894\n",
      " 0.2574771  0.25448763 0.2528359  0.25179845 0.25201687 0.25454223\n",
      " 0.25102037 0.2537778  0.24984643 0.25323176 0.25260386 0.24880899\n",
      " 0.25051531 0.25186673 0.24659759 0.21585651 0.22042945 0.2419018\n",
      " 0.21457335 0.23629141 0.24405859 0.23491271 0.19834282 0.24486397\n",
      " 0.24388114 0.24643379 0.20799378 0.23085849 0.23945835 0.2466249\n",
      " 0.23915803 0.20448558 0.23777932 0.23934914 0.24359447 0.20908582\n",
      " 0.19740093 0.21208894 0.23925358 0.23525397 0.22872899]\n",
      "  loss: [2.23523474 2.18314528 2.12895465 2.09570742 2.08201933 2.07737327\n",
      " 2.07005548 2.06574678 2.06261873 2.05912757 2.05672336 2.05210519\n",
      " 2.04938149 2.04759932 2.04132438 2.03669524 2.0284586  2.02685046\n",
      " 2.0176878  2.01153016 2.00771809 2.00207853 2.00506186 1.99974084\n",
      " 2.00069952 1.99651206 1.99366653 1.99697042 1.99219847 1.99218392\n",
      " 1.99274409 1.99319732 1.99287355 1.99326169 1.99192357 1.98948932\n",
      " 1.99009264 1.99141204 1.98809004 1.99229407 1.99202144 1.98934913\n",
      " 1.99022448 1.99224389 1.99255335 1.9941256  1.99405754 1.99625158\n",
      " 1.99877524 1.99659312 2.00285411 1.99859333 2.00530958 2.01426983\n",
      " 2.01451278 2.0088594  2.00968575 2.01891994 2.02884579 2.02142286\n",
      " 2.00327325 2.00942039 2.01818514 2.02938724 2.02045107 2.01116776\n",
      " 2.02755189 2.01253581 2.02468276 2.01635098 2.02196026 2.02741599\n",
      " 2.02619386 2.01973557 2.03891158 2.15750647 2.14742923 2.06162262\n",
      " 2.16750479 2.08591604 2.05704927 2.08036542 2.21514893 2.04669595\n",
      " 2.05128169 2.03951216 2.19003749 2.10219836 2.07284808 2.03991318\n",
      " 2.0691154  2.19993496 2.07716751 2.07423997 2.04908061 2.189399\n",
      " 2.2161479  2.16912174 2.06529927 2.09144592 2.11161757]\n",
      "  val_accuracy: [0.19898586 0.2198448  0.22768131 0.2400507  0.24174093 0.23958974\n",
      " 0.24423786 0.24400738 0.24454518 0.24200983 0.24635065 0.23025507\n",
      " 0.23056239 0.24577443 0.24719577 0.24969269 0.22034419 0.2510756\n",
      " 0.24028119 0.25165182 0.25664568 0.25968039 0.25430238 0.26198524\n",
      " 0.26044866 0.26194683 0.25814381 0.25572371 0.26356024 0.25391826\n",
      " 0.26106331 0.26190841 0.25979564 0.26382914 0.26171634 0.26559618\n",
      " 0.26567301 0.26259989 0.26532727 0.2664029  0.2222649  0.26574984\n",
      " 0.26836202 0.2632145  0.25825906 0.25441763 0.25453287 0.26225415\n",
      " 0.26060233 0.26175475 0.23586355 0.26064074 0.26467425 0.26240781\n",
      " 0.2637139  0.25572371 0.25895053 0.26367548 0.25341886 0.17977874\n",
      " 0.23751536 0.26202366 0.25591579 0.25476336 0.25883529 0.25714505\n",
      " 0.24873233 0.26259989 0.254264   0.25487861 0.26236939 0.25115243\n",
      " 0.22883375 0.26286876 0.25799015 0.21538875 0.25499386 0.19633529\n",
      " 0.20693761 0.26163954 0.23037031 0.19648893 0.22783497 0.22068992\n",
      " 0.21888445 0.24304701 0.22345574 0.21892287 0.23401967 0.23705439\n",
      " 0.19675784 0.22030577 0.2594499  0.2416257  0.20190534 0.21492778\n",
      " 0.21170099 0.21834666 0.21120159 0.20113707 0.1748233 ]\n",
      "  val_loss: [2.20405245 2.15148354 2.08515191 2.06399441 2.052809   2.05481863\n",
      " 2.05542111 2.04128671 2.03745246 2.03970051 2.03667116 2.0670743\n",
      " 2.05808759 2.02512288 2.03340054 2.01811767 2.09242678 2.0211854\n",
      " 2.04208088 2.0096817  2.00526452 2.00229788 2.02267361 1.99978435\n",
      " 2.00482893 1.9986403  1.9982667  2.01337218 1.99221826 2.03083539\n",
      " 1.99758565 2.00260878 2.01029754 1.99155557 1.99726319 1.98904157\n",
      " 1.98895288 1.99613571 1.99222457 1.99063849 2.12151027 1.99337173\n",
      " 1.98633993 1.99713945 2.01764226 2.02098203 2.0250771  1.99653351\n",
      " 2.00595236 2.01077271 2.12771487 2.00841713 1.99065292 2.00406075\n",
      " 1.99079335 2.00317621 2.01636696 1.99924493 2.04944181 2.27840853\n",
      " 2.12764573 2.00256991 2.01952171 2.02762771 1.99920309 2.00376606\n",
      " 2.07741642 2.00703359 2.03434134 2.01903963 1.99597251 2.03463292\n",
      " 2.12896347 1.99925661 2.0136869  2.17275095 2.02595305 2.22753835\n",
      " 2.2006824  2.00806689 2.09997153 2.22806811 2.13370538 2.14046597\n",
      " 2.21804166 2.07229519 2.15180182 2.15051341 2.08584595 2.09363842\n",
      " 2.22877121 2.16138625 2.01515007 2.06133246 2.23019433 2.17717576\n",
      " 2.18688273 2.1467092  2.18954515 2.21680689 2.26373053]\n",
      "Running: Activation=LSGELUS150 at 2025-09-28 04:33:37\n",
      "Epoch 1/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2592 - loss: 2.0522 - val_accuracy: 0.4720 - val_loss: 1.5893\n",
      "Epoch 2/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5425 - loss: 1.3740 - val_accuracy: 0.5962 - val_loss: 1.2942\n",
      "Epoch 3/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6278 - loss: 1.1712 - val_accuracy: 0.6054 - val_loss: 1.2832\n",
      "Epoch 4/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6571 - loss: 1.0809 - val_accuracy: 0.6363 - val_loss: 1.1844\n",
      "Epoch 5/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6781 - loss: 1.0296 - val_accuracy: 0.6560 - val_loss: 1.1301\n",
      "Epoch 6/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6867 - loss: 0.9977 - val_accuracy: 0.6595 - val_loss: 1.1262\n",
      "Epoch 7/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6974 - loss: 0.9631 - val_accuracy: 0.6482 - val_loss: 1.1591\n",
      "Epoch 8/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7045 - loss: 0.9485 - val_accuracy: 0.6819 - val_loss: 1.0417\n",
      "Epoch 9/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7087 - loss: 0.9336 - val_accuracy: 0.6688 - val_loss: 1.0751\n",
      "Epoch 10/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7159 - loss: 0.9125 - val_accuracy: 0.6734 - val_loss: 1.0669\n",
      "Epoch 11/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7184 - loss: 0.9006 - val_accuracy: 0.6903 - val_loss: 1.0141\n",
      "Epoch 12/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7189 - loss: 0.8957 - val_accuracy: 0.6989 - val_loss: 0.9914\n",
      "Epoch 13/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7221 - loss: 0.8780 - val_accuracy: 0.6915 - val_loss: 1.0106\n",
      "Epoch 14/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7282 - loss: 0.8690 - val_accuracy: 0.6878 - val_loss: 1.0086\n",
      "Epoch 15/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7317 - loss: 0.8521 - val_accuracy: 0.6947 - val_loss: 0.9871\n",
      "Epoch 16/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7331 - loss: 0.8482 - val_accuracy: 0.6913 - val_loss: 1.0158\n",
      "Epoch 17/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7357 - loss: 0.8453 - val_accuracy: 0.7075 - val_loss: 0.9607\n",
      "Epoch 18/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7361 - loss: 0.8354 - val_accuracy: 0.6991 - val_loss: 0.9850\n",
      "Epoch 19/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7350 - loss: 0.8450 - val_accuracy: 0.7027 - val_loss: 0.9777\n",
      "Epoch 20/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7383 - loss: 0.8371 - val_accuracy: 0.7038 - val_loss: 0.9621\n",
      "Epoch 21/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7406 - loss: 0.8254 - val_accuracy: 0.7077 - val_loss: 0.9509\n",
      "Epoch 22/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7412 - loss: 0.8235 - val_accuracy: 0.7139 - val_loss: 0.9373\n",
      "Epoch 23/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7432 - loss: 0.8185 - val_accuracy: 0.7015 - val_loss: 0.9781\n",
      "Epoch 24/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7462 - loss: 0.8096 - val_accuracy: 0.7210 - val_loss: 0.9173\n",
      "Epoch 25/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7481 - loss: 0.8035 - val_accuracy: 0.7220 - val_loss: 0.9180\n",
      "Epoch 26/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7502 - loss: 0.7952 - val_accuracy: 0.7169 - val_loss: 0.9206\n",
      "Epoch 27/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7510 - loss: 0.7972 - val_accuracy: 0.7215 - val_loss: 0.9199\n",
      "Epoch 28/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7515 - loss: 0.7954 - val_accuracy: 0.7226 - val_loss: 0.9135\n",
      "Epoch 29/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7552 - loss: 0.7841 - val_accuracy: 0.7178 - val_loss: 0.9297\n",
      "Epoch 30/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7526 - loss: 0.7885 - val_accuracy: 0.7103 - val_loss: 0.9510\n",
      "Epoch 31/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7542 - loss: 0.7862 - val_accuracy: 0.6998 - val_loss: 0.9710\n",
      "Epoch 32/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7513 - loss: 0.7868 - val_accuracy: 0.7175 - val_loss: 0.9260\n",
      "Epoch 33/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7542 - loss: 0.7894 - val_accuracy: 0.7281 - val_loss: 0.8948\n",
      "Epoch 34/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7535 - loss: 0.7900 - val_accuracy: 0.7230 - val_loss: 0.9148\n",
      "Epoch 35/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7564 - loss: 0.7781 - val_accuracy: 0.7194 - val_loss: 0.9221\n",
      "Epoch 36/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7558 - loss: 0.7756 - val_accuracy: 0.7157 - val_loss: 0.9418\n",
      "Epoch 37/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7613 - loss: 0.7649 - val_accuracy: 0.7130 - val_loss: 0.9477\n",
      "Epoch 38/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.7658 - val_accuracy: 0.7079 - val_loss: 0.9506\n",
      "Epoch 39/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7588 - loss: 0.7682 - val_accuracy: 0.7273 - val_loss: 0.9086\n",
      "Epoch 40/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7649 - loss: 0.7526 - val_accuracy: 0.7331 - val_loss: 0.8843\n",
      "Epoch 41/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7607 - loss: 0.7647 - val_accuracy: 0.7252 - val_loss: 0.9011\n",
      "Epoch 42/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7623 - loss: 0.7610 - val_accuracy: 0.7225 - val_loss: 0.9090\n",
      "Epoch 43/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.7673 - val_accuracy: 0.7313 - val_loss: 0.8930\n",
      "Epoch 44/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.7765 - val_accuracy: 0.7006 - val_loss: 0.9689\n",
      "Epoch 45/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7636 - loss: 0.7552 - val_accuracy: 0.7181 - val_loss: 0.9210\n",
      "Epoch 46/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7652 - loss: 0.7552 - val_accuracy: 0.7159 - val_loss: 0.9436\n",
      "Epoch 47/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7652 - loss: 0.7494 - val_accuracy: 0.7064 - val_loss: 0.9653\n",
      "Epoch 48/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7652 - loss: 0.7565 - val_accuracy: 0.7183 - val_loss: 0.9270\n",
      "Epoch 49/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.7550 - val_accuracy: 0.7248 - val_loss: 0.9121\n",
      "Epoch 50/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7685 - loss: 0.7468 - val_accuracy: 0.7280 - val_loss: 0.8979\n",
      "Epoch 51/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7660 - loss: 0.7535 - val_accuracy: 0.7346 - val_loss: 0.8811\n",
      "Epoch 52/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7626 - loss: 0.7562 - val_accuracy: 0.7236 - val_loss: 0.9128\n",
      "Epoch 53/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7650 - loss: 0.7441 - val_accuracy: 0.7183 - val_loss: 0.9260\n",
      "Epoch 54/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.7374 - val_accuracy: 0.7328 - val_loss: 0.8877\n",
      "Epoch 55/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7688 - loss: 0.7475 - val_accuracy: 0.7144 - val_loss: 0.9516\n",
      "Epoch 56/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7661 - loss: 0.7481 - val_accuracy: 0.7357 - val_loss: 0.8843\n",
      "Epoch 57/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7681 - loss: 0.7473 - val_accuracy: 0.7297 - val_loss: 0.8948\n",
      "Epoch 58/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7669 - loss: 0.7437 - val_accuracy: 0.7337 - val_loss: 0.8831\n",
      "Epoch 59/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7711 - loss: 0.7340 - val_accuracy: 0.7331 - val_loss: 0.8919\n",
      "Epoch 60/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.7464 - val_accuracy: 0.7023 - val_loss: 0.9664\n",
      "Epoch 61/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7672 - loss: 0.7428 - val_accuracy: 0.7315 - val_loss: 0.8854\n",
      "Epoch 62/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7664 - loss: 0.7410 - val_accuracy: 0.7162 - val_loss: 0.9484\n",
      "Epoch 63/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7723 - loss: 0.7359 - val_accuracy: 0.7184 - val_loss: 0.9280\n",
      "Epoch 64/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7688 - loss: 0.7412 - val_accuracy: 0.7230 - val_loss: 0.9086\n",
      "Epoch 65/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7697 - loss: 0.7385 - val_accuracy: 0.7117 - val_loss: 0.9498\n",
      "Epoch 66/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.7388 - val_accuracy: 0.7252 - val_loss: 0.9059\n",
      "Epoch 67/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7699 - loss: 0.7419 - val_accuracy: 0.7277 - val_loss: 0.9008\n",
      "Epoch 68/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.7327 - val_accuracy: 0.7279 - val_loss: 0.9042\n",
      "Epoch 69/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.7386 - val_accuracy: 0.7400 - val_loss: 0.8712\n",
      "Epoch 70/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7727 - loss: 0.7268 - val_accuracy: 0.7242 - val_loss: 0.9079\n",
      "Epoch 71/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7744 - loss: 0.7233 - val_accuracy: 0.7202 - val_loss: 0.9323\n",
      "Epoch 72/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7745 - loss: 0.7244 - val_accuracy: 0.7180 - val_loss: 0.9298\n",
      "Epoch 73/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.7346 - val_accuracy: 0.7266 - val_loss: 0.9152\n",
      "Epoch 74/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7707 - loss: 0.7370 - val_accuracy: 0.7331 - val_loss: 0.8900\n",
      "Epoch 75/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7740 - loss: 0.7258 - val_accuracy: 0.7365 - val_loss: 0.8802\n",
      "Epoch 76/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7715 - loss: 0.7332 - val_accuracy: 0.7290 - val_loss: 0.8919\n",
      "Epoch 77/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.7312 - val_accuracy: 0.7213 - val_loss: 0.9169\n",
      "Epoch 78/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7737 - loss: 0.7245 - val_accuracy: 0.7344 - val_loss: 0.8823\n",
      "Epoch 79/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7729 - loss: 0.7316 - val_accuracy: 0.7267 - val_loss: 0.9010\n",
      "Epoch 80/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7750 - loss: 0.7175 - val_accuracy: 0.7281 - val_loss: 0.9021\n",
      "Epoch 81/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7731 - loss: 0.7210 - val_accuracy: 0.7298 - val_loss: 0.8951\n",
      "Epoch 82/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.7332 - val_accuracy: 0.7210 - val_loss: 0.9105\n",
      "Epoch 83/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7702 - loss: 0.7345 - val_accuracy: 0.7274 - val_loss: 0.9050\n",
      "Epoch 84/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7761 - loss: 0.7186 - val_accuracy: 0.7163 - val_loss: 0.9310\n",
      "Epoch 85/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7731 - loss: 0.7251 - val_accuracy: 0.7295 - val_loss: 0.9038\n",
      "Epoch 86/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7738 - loss: 0.7263 - val_accuracy: 0.7184 - val_loss: 0.9196\n",
      "Epoch 87/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7752 - loss: 0.7251 - val_accuracy: 0.7244 - val_loss: 0.9160\n",
      "Epoch 88/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7760 - loss: 0.7216 - val_accuracy: 0.7306 - val_loss: 0.9112\n",
      "Epoch 89/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7762 - loss: 0.7193 - val_accuracy: 0.7299 - val_loss: 0.8953\n",
      "Epoch 90/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7750 - loss: 0.7231 - val_accuracy: 0.7291 - val_loss: 0.9063\n",
      "Epoch 91/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7748 - loss: 0.7206 - val_accuracy: 0.7241 - val_loss: 0.9144\n",
      "Epoch 92/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7781 - loss: 0.7091 - val_accuracy: 0.7206 - val_loss: 0.9305\n",
      "Epoch 93/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7761 - loss: 0.7173 - val_accuracy: 0.7312 - val_loss: 0.8895\n",
      "Epoch 94/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7787 - loss: 0.7097 - val_accuracy: 0.7228 - val_loss: 0.9177\n",
      "Epoch 95/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7757 - loss: 0.7139 - val_accuracy: 0.7139 - val_loss: 0.9477\n",
      "Epoch 96/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7729 - loss: 0.7264 - val_accuracy: 0.7326 - val_loss: 0.8902\n",
      "Epoch 97/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7776 - loss: 0.7147 - val_accuracy: 0.7261 - val_loss: 0.9097\n",
      "Epoch 98/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7766 - loss: 0.7121 - val_accuracy: 0.7335 - val_loss: 0.8947\n",
      "Epoch 99/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7755 - loss: 0.7180 - val_accuracy: 0.7293 - val_loss: 0.9027\n",
      "Epoch 100/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7776 - loss: 0.7137 - val_accuracy: 0.7281 - val_loss: 0.8995\n",
      "Epoch 101/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7785 - loss: 0.7124 - val_accuracy: 0.7367 - val_loss: 0.8809\n",
      "Running: Activation=LSGELUS150 at 2025-09-28 04:38:21\n",
      "Details in results for round 000:\n",
      "\n",
      "Activation: GELU\n",
      "  accuracy: [0.18821409 0.20531826 0.21425939 0.22205387 0.22804646 0.23040801\n",
      " 0.23256481 0.23432574 0.2347489  0.2360184  0.23693299 0.240605\n",
      " 0.24068689 0.24019548 0.24343066 0.24508238 0.24596967 0.24521889\n",
      " 0.24859057 0.25153911 0.25309527 0.25428286 0.25429651 0.25570253\n",
      " 0.25622123 0.25821424 0.25889677 0.25900596 0.25900596 0.26052117\n",
      " 0.26057577 0.25987959 0.26091704 0.26086244 0.26048023 0.2612583\n",
      " 0.26139483 0.26043928 0.26248688 0.26108086 0.26151767 0.260685\n",
      " 0.26157227 0.26053482 0.26158592 0.25987959 0.25949737 0.25891042\n",
      " 0.25805041 0.26016626 0.25709489 0.25811869 0.25701299 0.25316352\n",
      " 0.25265846 0.25454223 0.25623488 0.25297242 0.25084293 0.25138894\n",
      " 0.2574771  0.25448763 0.2528359  0.25179845 0.25201687 0.25454223\n",
      " 0.25102037 0.2537778  0.24984643 0.25323176 0.25260386 0.24880899\n",
      " 0.25051531 0.25186673 0.24659759 0.21585651 0.22042945 0.2419018\n",
      " 0.21457335 0.23629141 0.24405859 0.23491271 0.19834282 0.24486397\n",
      " 0.24388114 0.24643379 0.20799378 0.23085849 0.23945835 0.2466249\n",
      " 0.23915803 0.20448558 0.23777932 0.23934914 0.24359447 0.20908582\n",
      " 0.19740093 0.21208894 0.23925358 0.23525397 0.22872899]\n",
      "  loss: [2.23523474 2.18314528 2.12895465 2.09570742 2.08201933 2.07737327\n",
      " 2.07005548 2.06574678 2.06261873 2.05912757 2.05672336 2.05210519\n",
      " 2.04938149 2.04759932 2.04132438 2.03669524 2.0284586  2.02685046\n",
      " 2.0176878  2.01153016 2.00771809 2.00207853 2.00506186 1.99974084\n",
      " 2.00069952 1.99651206 1.99366653 1.99697042 1.99219847 1.99218392\n",
      " 1.99274409 1.99319732 1.99287355 1.99326169 1.99192357 1.98948932\n",
      " 1.99009264 1.99141204 1.98809004 1.99229407 1.99202144 1.98934913\n",
      " 1.99022448 1.99224389 1.99255335 1.9941256  1.99405754 1.99625158\n",
      " 1.99877524 1.99659312 2.00285411 1.99859333 2.00530958 2.01426983\n",
      " 2.01451278 2.0088594  2.00968575 2.01891994 2.02884579 2.02142286\n",
      " 2.00327325 2.00942039 2.01818514 2.02938724 2.02045107 2.01116776\n",
      " 2.02755189 2.01253581 2.02468276 2.01635098 2.02196026 2.02741599\n",
      " 2.02619386 2.01973557 2.03891158 2.15750647 2.14742923 2.06162262\n",
      " 2.16750479 2.08591604 2.05704927 2.08036542 2.21514893 2.04669595\n",
      " 2.05128169 2.03951216 2.19003749 2.10219836 2.07284808 2.03991318\n",
      " 2.0691154  2.19993496 2.07716751 2.07423997 2.04908061 2.189399\n",
      " 2.2161479  2.16912174 2.06529927 2.09144592 2.11161757]\n",
      "  val_accuracy: [0.19898586 0.2198448  0.22768131 0.2400507  0.24174093 0.23958974\n",
      " 0.24423786 0.24400738 0.24454518 0.24200983 0.24635065 0.23025507\n",
      " 0.23056239 0.24577443 0.24719577 0.24969269 0.22034419 0.2510756\n",
      " 0.24028119 0.25165182 0.25664568 0.25968039 0.25430238 0.26198524\n",
      " 0.26044866 0.26194683 0.25814381 0.25572371 0.26356024 0.25391826\n",
      " 0.26106331 0.26190841 0.25979564 0.26382914 0.26171634 0.26559618\n",
      " 0.26567301 0.26259989 0.26532727 0.2664029  0.2222649  0.26574984\n",
      " 0.26836202 0.2632145  0.25825906 0.25441763 0.25453287 0.26225415\n",
      " 0.26060233 0.26175475 0.23586355 0.26064074 0.26467425 0.26240781\n",
      " 0.2637139  0.25572371 0.25895053 0.26367548 0.25341886 0.17977874\n",
      " 0.23751536 0.26202366 0.25591579 0.25476336 0.25883529 0.25714505\n",
      " 0.24873233 0.26259989 0.254264   0.25487861 0.26236939 0.25115243\n",
      " 0.22883375 0.26286876 0.25799015 0.21538875 0.25499386 0.19633529\n",
      " 0.20693761 0.26163954 0.23037031 0.19648893 0.22783497 0.22068992\n",
      " 0.21888445 0.24304701 0.22345574 0.21892287 0.23401967 0.23705439\n",
      " 0.19675784 0.22030577 0.2594499  0.2416257  0.20190534 0.21492778\n",
      " 0.21170099 0.21834666 0.21120159 0.20113707 0.1748233 ]\n",
      "  val_loss: [2.20405245 2.15148354 2.08515191 2.06399441 2.052809   2.05481863\n",
      " 2.05542111 2.04128671 2.03745246 2.03970051 2.03667116 2.0670743\n",
      " 2.05808759 2.02512288 2.03340054 2.01811767 2.09242678 2.0211854\n",
      " 2.04208088 2.0096817  2.00526452 2.00229788 2.02267361 1.99978435\n",
      " 2.00482893 1.9986403  1.9982667  2.01337218 1.99221826 2.03083539\n",
      " 1.99758565 2.00260878 2.01029754 1.99155557 1.99726319 1.98904157\n",
      " 1.98895288 1.99613571 1.99222457 1.99063849 2.12151027 1.99337173\n",
      " 1.98633993 1.99713945 2.01764226 2.02098203 2.0250771  1.99653351\n",
      " 2.00595236 2.01077271 2.12771487 2.00841713 1.99065292 2.00406075\n",
      " 1.99079335 2.00317621 2.01636696 1.99924493 2.04944181 2.27840853\n",
      " 2.12764573 2.00256991 2.01952171 2.02762771 1.99920309 2.00376606\n",
      " 2.07741642 2.00703359 2.03434134 2.01903963 1.99597251 2.03463292\n",
      " 2.12896347 1.99925661 2.0136869  2.17275095 2.02595305 2.22753835\n",
      " 2.2006824  2.00806689 2.09997153 2.22806811 2.13370538 2.14046597\n",
      " 2.21804166 2.07229519 2.15180182 2.15051341 2.08584595 2.09363842\n",
      " 2.22877121 2.16138625 2.01515007 2.06133246 2.23019433 2.17717576\n",
      " 2.18688273 2.1467092  2.18954515 2.21680689 2.26373053]\n",
      "\n",
      "Activation: LSGELUS150\n",
      "  accuracy: [0.36784199 0.5743615  0.6354478  0.65685189 0.67801028 0.6872654\n",
      " 0.6963703  0.7039054  0.70940661 0.71616364 0.71808839 0.72008133\n",
      " 0.72200608 0.72573268 0.72993708 0.72981423 0.73412782 0.73511064\n",
      " 0.73605251 0.73901469 0.74147183 0.7424137  0.74361497 0.74541682\n",
      " 0.74861103 0.7482425  0.7499488  0.7499488  0.75307477 0.75291097\n",
      " 0.75454903 0.75342971 0.75332052 0.75457633 0.75639188 0.75611889\n",
      " 0.75959975 0.75822103 0.75718361 0.76040512 0.75848043 0.76078737\n",
      " 0.76125151 0.75964069 0.76302606 0.76414543 0.76273942 0.76402253\n",
      " 0.76246637 0.76576984 0.76415908 0.76475966 0.76456857 0.76759899\n",
      " 0.76641142 0.76772183 0.7672441  0.76833612 0.76766723 0.76890945\n",
      " 0.76821327 0.76781744 0.76967388 0.76768088 0.76878661 0.76959199\n",
      " 0.77062941 0.76938725 0.77012438 0.77062941 0.77106625 0.77168053\n",
      " 0.77106625 0.77202177 0.77019262 0.77095705 0.77053386 0.77277255\n",
      " 0.77346873 0.77350968 0.77181703 0.77239037 0.77146214 0.77563918\n",
      " 0.77236307 0.77337319 0.77405572 0.77442431 0.77461541 0.77510679\n",
      " 0.7747246  0.77442431 0.77458811 0.77548903 0.77406937 0.77454716\n",
      " 0.77633536 0.77454716 0.77356428 0.77634901 0.77632171]\n",
      "  loss: [1.78700948 1.30085015 1.14834297 1.08492196 1.02619553 0.99779499\n",
      " 0.96905065 0.94663703 0.92823827 0.90938503 0.90150112 0.89184755\n",
      " 0.88358593 0.87487602 0.86087012 0.85914618 0.85059226 0.84337509\n",
      " 0.8419385  0.83135498 0.82491428 0.81987906 0.81698412 0.81132209\n",
      " 0.80510044 0.80115086 0.79922187 0.79682916 0.79194331 0.78936583\n",
      " 0.78606904 0.78564435 0.7862708  0.78362918 0.7768811  0.77721667\n",
      " 0.77161998 0.77167016 0.77296638 0.76989591 0.76975924 0.76563227\n",
      " 0.76575381 0.76820409 0.76077056 0.75801885 0.75800252 0.7585277\n",
      " 0.75692165 0.75692666 0.75694305 0.75393218 0.75272238 0.747338\n",
      " 0.75153244 0.74884403 0.74933958 0.74439842 0.74634314 0.74342597\n",
      " 0.74173218 0.74231672 0.74149019 0.74475819 0.74149626 0.74020398\n",
      " 0.73818672 0.7360636  0.73261815 0.73419416 0.73253506 0.73423368\n",
      " 0.73583472 0.73199129 0.73542464 0.73459178 0.73661    0.73064959\n",
      " 0.72879952 0.72723734 0.72911185 0.73146009 0.72988147 0.72421908\n",
      " 0.72822666 0.72720402 0.72709572 0.72600943 0.72484159 0.72246319\n",
      " 0.72530472 0.72244352 0.72030568 0.72104406 0.72426909 0.72400582\n",
      " 0.71840775 0.7205407  0.72160912 0.7158125  0.71976447]\n",
      "  val_accuracy: [0.47199601 0.59615088 0.60537034 0.63629377 0.65600032 0.65953439\n",
      " 0.64820224 0.6818915  0.66883069 0.6734404  0.69034266 0.69894743\n",
      " 0.69153351 0.68776888 0.69472188 0.6913414  0.70747542 0.69910109\n",
      " 0.70271206 0.70378762 0.7077443  0.71385217 0.70148277 0.72099721\n",
      " 0.72195756 0.71688694 0.7214582  0.72264904 0.71777046 0.71027964\n",
      " 0.69983095 0.71750152 0.72814226 0.72295636 0.71942222 0.71565765\n",
      " 0.71304548 0.70789796 0.72725874 0.73309773 0.72522283 0.72245699\n",
      " 0.73133069 0.70056087 0.71807778 0.71592653 0.7063998  0.71830821\n",
      " 0.72476184 0.72802705 0.73463428 0.72364783 0.71834666 0.73282886\n",
      " 0.71438998 0.73567146 0.72967887 0.73371238 0.73309773 0.70232791\n",
      " 0.73148435 0.71615702 0.71842349 0.72303319 0.71173942 0.72522283\n",
      " 0.72768134 0.72787338 0.7399739  0.72422403 0.72015214 0.7179625\n",
      " 0.72656733 0.73305929 0.73651659 0.72902584 0.72130454 0.7343654\n",
      " 0.72672093 0.72810388 0.72983253 0.72103566 0.7274124  0.71631068\n",
      " 0.72952521 0.71842349 0.72437769 0.73060077 0.72987092 0.72910267\n",
      " 0.7241472  0.72057468 0.73121542 0.72284114 0.71385217 0.73263675\n",
      " 0.7260679  0.73348188 0.7293331  0.72806543 0.73670864]\n",
      "  val_loss: [1.58925223 1.29421997 1.28324103 1.18435669 1.13005507 1.12622607\n",
      " 1.15912223 1.04173601 1.0750792  1.06691658 1.01408517 0.99142927\n",
      " 1.01064932 1.00864422 0.98707122 1.01579213 0.96068054 0.98495317\n",
      " 0.97771108 0.96208131 0.95094407 0.93726116 0.97812474 0.91727436\n",
      " 0.91803432 0.92063278 0.91992474 0.9134798  0.92965919 0.95097357\n",
      " 0.97101492 0.92601949 0.89482701 0.91478401 0.92207623 0.94184113\n",
      " 0.94770581 0.9506374  0.90864801 0.88429445 0.90112191 0.90896028\n",
      " 0.89298844 0.96892232 0.92102689 0.94356591 0.96532613 0.92701566\n",
      " 0.91205579 0.89785671 0.88111717 0.91283137 0.92596853 0.88772148\n",
      " 0.95162791 0.88431174 0.89480239 0.88305682 0.89185971 0.96641701\n",
      " 0.88538921 0.94842118 0.92800164 0.90858972 0.94980216 0.90594333\n",
      " 0.90077925 0.90421808 0.87116563 0.90788704 0.93233633 0.92980605\n",
      " 0.91523743 0.89002216 0.8802278  0.89186376 0.91689092 0.8823086\n",
      " 0.9009856  0.90209943 0.89505655 0.91045702 0.90503824 0.93098038\n",
      " 0.90379894 0.91958344 0.91602415 0.9111529  0.89526516 0.90625405\n",
      " 0.91442591 0.93046814 0.88950205 0.91770172 0.94771284 0.89019436\n",
      " 0.90972775 0.8947432  0.90267807 0.89953285 0.88088667]\n",
      "Running: Activation=LSGELUS180 at 2025-09-28 04:38:21\n",
      "Epoch 1/101\n",
      "\u001b[1m1145/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2123 - loss: 2.1849 - val_accuracy: 0.4308 - val_loss: 1.6626\n",
      "Epoch 2/101\n",
      "\u001b[1m 771/1145\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4664 - loss: 1.5603"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 242\u001b[39m\n\u001b[32m    237\u001b[39m model = build_model(act_fn)\n\u001b[32m    239\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m#model.compile(optimizer='adam',\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m#              loss='categorical_crossentropy',\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m#              metrics=['accuracy'])\u001b[39;00m\n\u001b[32m    247\u001b[39m \n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Training model\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m#history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning: Activation=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.datetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)   \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator.enumerate_epoch():\n\u001b[32m    319\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     logs = \u001b[38;5;28mself\u001b[39m._pythonify_logs(logs)\n\u001b[32m    322\u001b[39m     callbacks.on_train_batch_end(step, logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1550\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1552\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1560\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1561\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1562\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1567\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#--- 2025-09-28 03-18 – by Dr. Thawatchai Chomsiri\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "#from tensorflow.keras.datasets import fashion_mnist\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "#def lsgelu(x):    # Left-Shifted GELU with 1 range\n",
    "#    return x * 0.5 * (1 + tf.math.erf((x + 1.5) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus300(x):    \n",
    "    S=3.00\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus270(x):    \n",
    "    S=2.70\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus240(x):    \n",
    "    S=2.40\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus210(x):    \n",
    "    S=2.10\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus180(x):    \n",
    "    S=1.80\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus150(x):    # LSGELU   \n",
    "    S=1.50\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus120(x):    \n",
    "    S=1.20\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus090(x):    \n",
    "    S=0.90\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus060(x):    \n",
    "    S=0.60\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus030(x):    \n",
    "    S=0.30\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelus000(x):    # GELU    \n",
    "    S=0.00\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def lsgelu9999(x):    \n",
    "    S=3.71901648546\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu9950(x):    \n",
    "    S=2.57582930355\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu9900(x):    \n",
    "    S=2.32634787404\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu9750(x):    \n",
    "    S=1.95996398454\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu9500(x):    \n",
    "    S=1.64485362695\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "#-----\n",
    "def lsgelu9332(x): # LSGELU   \n",
    "    S=1.5\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "#-----\n",
    "def lsgelu9250(x):    \n",
    "    S=1.43953147094\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu9000(x):    \n",
    "    S=1.28155156554\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu8000(x):    \n",
    "    S=0.841621233573\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu7500(x):    \n",
    "    S=0.674489750196\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "def lsgelu6666(x):    \n",
    "    S=0.430727299295\n",
    "    return x * 0.5 * (1 + tf.math.erf((x + S) / tf.sqrt(2.0)))\n",
    "\n",
    "#-----\n",
    "def lsgelu2(x):    # Left-Shifted GELU with 2 range\n",
    "    return tf.where(\n",
    "        x >= 0,\n",
    "        x, \n",
    "        x * 0.5 * (1 + tf.math.erf((x + 1.5) / tf.sqrt(2.0)))\n",
    "    ) \n",
    "\n",
    "def lsgelu3(x):    # Left-Shifted GELU with 3 range\n",
    "    L = -3.00\n",
    "    return tf.where(\n",
    "        x >= 0,\n",
    "        x,\n",
    "        tf.where(\n",
    "            x >= L,\n",
    "            x * 0.5 * (1 + tf.math.erf((x + 1.5) / tf.sqrt(2.0))),\n",
    "            tf.zeros_like(x)\n",
    "        )\n",
    "    ) \n",
    "\n",
    "\n",
    "def build_model(activation_fn):\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=activation_fn)(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "# โหลดข้อมูลจากไฟล์ .mat\n",
    "train_data = sio.loadmat('../train_32x32.mat')\n",
    "test_data = sio.loadmat('../test_32x32.mat')\n",
    "\n",
    "# ข้อมูลในไฟล์ .mat อยู่ใน key 'X' สำหรับภาพ, 'y' สำหรับ labels\n",
    "\n",
    "X_train = train_data['X']\n",
    "Y_train = train_data['y']\n",
    "X_test = test_data['X']\n",
    "Y_test = test_data['y']\n",
    "\n",
    "# แปลงรูปภาพให้เป็น float32 และปรับขนาดตามต้องการ\n",
    "X_train = np.transpose(X_train, (3, 0, 1, 2)).astype('float32') / 255.0\n",
    "X_test = np.transpose(X_test, (3, 0, 1, 2)).astype('float32') / 255.0\n",
    "\n",
    "# แปลง labels จาก 1-10 เป็น 0-9 (ตามแนวทางปกติ)\n",
    "Y_train = Y_train.flatten()\n",
    "Y_test = Y_test.flatten()\n",
    "\n",
    "# บางครั้ง label 10 แทน 0 ให้เปลี่ยนให้ตรง\n",
    "Y_train[Y_train == 10] = 0\n",
    "Y_test[Y_test == 10] = 0\n",
    "\n",
    "# ถ้าต้องการใช้ one-hot encoding ก็ใช้\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train, 10)\n",
    "Y_test = to_categorical(Y_test, 10)\n",
    "\n",
    "activations_list = {\n",
    "#    \"LSGELUS300\": lsgelus300,\n",
    "#    \"LSGELUS270\": lsgelus270,\n",
    "#    \"LSGELUS240\": lsgelus240,\n",
    "#    \"LSGELUS210\": lsgelus210,\n",
    "#    \"LSGELUS180\": lsgelus180,\n",
    "#    \"LSGELUS150\": lsgelus150,  # LSGELU\n",
    "#    \"LSGELUS120\": lsgelus120,\n",
    "#    \"LSGELUS090\": lsgelus090,\n",
    "#    \"LSGELUS060\": lsgelus060,\n",
    "#    \"LSGELUS030\": lsgelus030,  \n",
    "#    'GELU': tf.nn.gelu,\n",
    "#    'ELU': tf.nn.elu,\n",
    "#    'ReLU': tf.nn.relu,\n",
    "#    'Swish': tf.nn.swish,     \n",
    "\n",
    "\n",
    "    'GELU': tf.nn.gelu,\n",
    "    \"LSGELUS150\": lsgelus150,  # LSGELU\n",
    "    \"LSGELUS180\": lsgelus180,\n",
    "    \"LSGELUS120\": lsgelus120,\n",
    "    'ELU': tf.nn.elu,\n",
    "    'ReLU': tf.nn.relu,\n",
    "    'Swish': tf.nn.swish,     \n",
    "    \"LSGELUS210\": lsgelus210,\n",
    "    \"LSGELUS090\": lsgelus090,\n",
    "    \"LSGELUS240\": lsgelus240,\n",
    "    \"LSGELUS060\": lsgelus060,\n",
    "    \"LSGELUS270\": lsgelus270,\n",
    "\n",
    "    \"LSGELUS300\": lsgelus300,  \n",
    "    \"LSGELUS030\": lsgelus030,\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "epochs = 101  ################\n",
    "num_runs = 10 ###############\n",
    "batch_size = 64 ###############\n",
    "results = {\n",
    "    'activation': [],\n",
    "    'accuracy_per_epoch': []\n",
    "}\n",
    "accuracy_summary = {}\n",
    "\n",
    "for run_idx in range(num_runs):\n",
    "    print(f\"\\n--- Run {run_idx:03d} of {num_runs:03d} ---\")\n",
    "    results = {}\n",
    "    accuracy_results = {}\n",
    "    loss_results = {}\n",
    "    \n",
    "    for act_name, act_fn in activations_list.items():\n",
    "        print(f\"Running: Activation={act_name} at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        model = build_model(act_fn)\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        history = model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test), batch_size=batch_size)\n",
    "\n",
    "        #model.compile(optimizer='adam',\n",
    "        #              loss='categorical_crossentropy',\n",
    "        #              metrics=['accuracy'])\n",
    "        \n",
    "        # Training model\n",
    "        #history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "        \n",
    "        print(f\"Running: Activation={act_name} at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")   \n",
    "        results[act_name] = {key: np.array(val) for key, val in history.history.items()}\n",
    "        print(f\"Details in results for round {run_idx:03d}:\")\n",
    "        for act_name, metrics_dict in results.items():\n",
    "            print(f\"\\nActivation: {act_name}\")\n",
    "            for metric_name, metric_values in metrics_dict.items():\n",
    "                print(f\"  {metric_name}: {metric_values}\")\n",
    "\n",
    "        np.savez(f\"accuracy_{run_idx:03d}_{act_name}.npz\", accuracy=np.array(history.history['accuracy']))\n",
    "        np.savez(f\"loss_{run_idx:03d}_{act_name}.npz\", loss=np.array(history.history['loss']))\n",
    "        \n",
    "        accuracy_results[act_name] = np.array(history.history['accuracy'])\n",
    "        loss_results[act_name] = np.array(history.history['loss'])\n",
    "\n",
    "    print(f\" ----- Data -------- \")\n",
    "#    results[act_name] = {key: np.array(val) for key, val in history.history.items()}\n",
    "#    print(f\"Details in results for round {run_idx:03d}:\")\n",
    "#    for act_name, metrics_dict in results.items():\n",
    "#        print(f\"\\nActivation: {act_name}\")\n",
    "#        for metric_name, metric_values in metrics_dict.items():\n",
    "#            print(f\"  {metric_name}: {metric_values}\")\n",
    "\n",
    "    print(f\" ------------------- \")\n",
    "\n",
    "\n",
    "   \n",
    "print(f\"\\nEND at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b91e6-846f-4210-8763-981ae70b6912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
